{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.viewer import ImageViewer\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import feature\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmar\\AppData\\Local\\Temp\\ipykernel_10404\\2644223124.py:10: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n"
     ]
    }
   ],
   "source": [
    "image = imread(\"C:/Users/emmar/Documents/GitHub/VISN/Opdracht_1/flower.jpg\")\n",
    "\n",
    "#grayscale\n",
    "image_gray = rgb2gray(image)\n",
    "\n",
    "#edge detection\n",
    "canny_filter = feature.canny(image_gray, sigma=2) #TODO change sigma\n",
    "\n",
    "#gaussian filter                    #for rgb image\n",
    "gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n",
    "\n",
    "# viewer = ImageViewer(image)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(image_gray)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(canny_filter)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(gaussian_filter)\n",
    "# viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the dataset\n",
    "Dataset used for this project from: https://www.kaggle.com/datasets/grassknoted/asl-alphabet?resource=download\n",
    "Tutorial: https://www.youtube.com/watch?v=j-3vuBynnOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"C:/Users/emmar/Documents/GitHub/VISN/Eindopdracht/dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for letter in letters:\n",
    "    #get directory of a certain letter\n",
    "    path = os.path.join(dataset_dir, letter)\n",
    "\n",
    "    #create number for each letter\n",
    "    letter_num = letters.index(letter)\n",
    "    print(letter)\n",
    "    \n",
    "    for image in os.listdir(path):\n",
    "        #get one image\n",
    "        image_array = cv2.imread(os.path.join(path, image), cv2.IMREAD_GRAYSCALE) #TODO gray scale can be added here, do it??\n",
    "\n",
    "        #compress the image to a smaller resolution\n",
    "        #TODO is this needed?? > makes faster\n",
    "        image_size = 50 #TODO this bigger/smaller?\n",
    "        compressed_image_array = cv2.resize(image_array, (image_size, image_size))\n",
    "\n",
    "        #add filter\n",
    "        canny_filter_image = feature.canny(compressed_image_array, sigma=3)\n",
    "\n",
    "        #add new image to the training set\n",
    "        training_data.append([canny_filter_image, letter_num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formating the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomize the images\n",
    "random.shuffle(training_data)\n",
    "\n",
    "plt.imshow(training_data[0][0])\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "test_data_size = 10000\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "#separate images and labels into the testing dataset\n",
    "#(the first 10.000) and the training dataset (the rest)\n",
    "\n",
    "#TODO can this be done easier? split?\n",
    "for i in range(len(training_data)):\n",
    "    if i < test_data_size:\n",
    "        test_images.append(training_data[i][0])\n",
    "        test_labels.append(training_data[i][1])\n",
    "    else:\n",
    "        train_images.append(training_data[i][0])\n",
    "        train_labels.append(training_data[i][1])\n",
    "\n",
    "#reshape train and test images\n",
    "train_images = np.array(train_images).reshape(-1, image_size, image_size, 1)\n",
    "test_images = np.array(test_images).reshape(-1, image_size, image_size, 1)\n",
    "\n",
    "#normalize the images\n",
    "# train_images = (train_images / 255) - 0.5\n",
    "# test_images = (test_images / 255) - 0.5\n",
    "\n",
    "#normalize the images canny\n",
    "train_images = train_images - 0.5\n",
    "test_images = test_images - 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emmar\\Documents\\GitHub\\VISN\\Eindopdracht\\src\\main.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     Conv2D(num_filters, filter_size, input_shape\u001b[39m=\u001b[39mtrain_images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     MaxPooling2D(pool_size\u001b[39m=\u001b[39mpool_size),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Flatten(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     Dense(\u001b[39m29\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdense\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_images, to_categorical(train_labels), epochs\u001b[39m=\u001b[39;49mnum_epochs, validation_data\u001b[39m=\u001b[39;49m(test_images, to_categorical(test_labels)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_images,  to_categorical(test_labels), verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(test_acc)\n",
      "File \u001b[1;32mc:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej_u4nbnr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n"
     ]
    }
   ],
   "source": [
    "num_filters = 3\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "num_epochs = 10 #TODO change these vars\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=train_images[0].shape),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(29, activation=\"sigmoid\", name=\"dense\")\n",
    "])\n",
    "\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, to_categorical(train_labels), epochs=num_epochs, validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
