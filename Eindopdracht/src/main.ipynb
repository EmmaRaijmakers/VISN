{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.viewer import ImageViewer\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import feature\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmar\\AppData\\Local\\Temp\\ipykernel_10404\\2644223124.py:10: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n"
     ]
    }
   ],
   "source": [
    "image = imread(\"C:/Users/emmar/Documents/GitHub/VISN/Opdracht_1/flower.jpg\")\n",
    "\n",
    "#grayscale\n",
    "image_gray = rgb2gray(image)\n",
    "\n",
    "#edge detection\n",
    "canny_filter = feature.canny(image_gray, sigma=2) #TODO change sigma\n",
    "\n",
    "#gaussian filter                    #for rgb image\n",
    "gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n",
    "\n",
    "# viewer = ImageViewer(image)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(image_gray)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(canny_filter)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(gaussian_filter)\n",
    "# viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the dataset\n",
    "Dataset used for this project from: https://www.kaggle.com/datasets/grassknoted/asl-alphabet?resource=download\n",
    "Tutorial: https://www.youtube.com/watch?v=j-3vuBynnOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"C:/Users/emmar/Documents/GitHub/VISN/Eindopdracht/dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for letter in letters[0]:\n",
    "    #get directory of a certain letter\n",
    "    path = os.path.join(dataset_dir, letter)\n",
    "\n",
    "    #create number for each letter\n",
    "    letter_num = letters.index(letter)\n",
    "    print(letter)\n",
    "    \n",
    "    for image in os.listdir(path):\n",
    "        #get one image\n",
    "        image_array = cv2.imread(os.path.join(path, image), cv2.IMREAD_GRAYSCALE) #TODO gray scale can be added here, do it??\n",
    "\n",
    "        #compress the image to a smaller resolution\n",
    "        #TODO is this needed?? > makes faster\n",
    "        image_size = 50 #TODO this bigger/smaller?\n",
    "        compressed_image_array = cv2.resize(image_array, (image_size, image_size))\n",
    "\n",
    "        #add filter\n",
    "        canny_filter_image = feature.canny(compressed_image_array, sigma=3)\n",
    "\n",
    "        #add new image to the training set\n",
    "        training_data.append([canny_filter_image, letter_num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formating the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomize the images\n",
    "random.shuffle(training_data)\n",
    "\n",
    "# print(training_data[0])\n",
    "\n",
    "plt.imshow(training_data[0][0])\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "test_data_size = 100\n",
    "#TODO reset this\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "#separate images and labels into the testing dataset\n",
    "#(the first 10.000) and the training dataset (the rest)\n",
    "\n",
    "#TODO can this be done easier? split?\n",
    "for i in range(len(training_data)):\n",
    "    if i < test_data_size:\n",
    "        test_images.append(training_data[i][0])\n",
    "        test_labels.append(training_data[i][1])\n",
    "    else:\n",
    "        train_images.append(training_data[i][0])\n",
    "        train_labels.append(training_data[i][1])\n",
    "\n",
    "#reshape train and test images\n",
    "train_images = np.array(train_images).reshape(-1, image_size, image_size, 1)\n",
    "test_images = np.array(test_images).reshape(-1, image_size, image_size, 1)\n",
    "\n",
    "#normalize the images\n",
    "# train_images = (train_images / 255) - 0.5\n",
    "# test_images = (test_images / 255) - 0.5\n",
    "\n",
    "#normalize the images canny\n",
    "train_images = train_images - 0.5\n",
    "test_images = test_images - 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emmar\\Documents\\GitHub\\VISN\\Eindopdracht\\src\\main.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     Conv2D(num_filters, filter_size, input_shape\u001b[39m=\u001b[39mtrain_images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     MaxPooling2D(pool_size\u001b[39m=\u001b[39mpool_size),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Flatten(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     Dense(\u001b[39m29\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdense\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_images, to_categorical(train_labels), epochs\u001b[39m=\u001b[39;49mnum_epochs, validation_data\u001b[39m=\u001b[39;49m(test_images, to_categorical(test_labels)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_images,  to_categorical(test_labels), verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emmar/Documents/GitHub/VISN/Eindopdracht/src/main.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(test_acc)\n",
      "File \u001b[1;32mc:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej_u4nbnr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\emmar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n"
     ]
    }
   ],
   "source": [
    "num_filters = 3\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "num_epochs = 10 #TODO change these vars\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=train_images[0].shape),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(29, activation=\"sigmoid\", name=\"dense\")\n",
    "])\n",
    "\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, to_categorical(train_labels), epochs=num_epochs, validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
