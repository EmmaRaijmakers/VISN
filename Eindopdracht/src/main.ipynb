{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.viewer import ImageViewer\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import feature\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmar\\AppData\\Local\\Temp\\ipykernel_15700\\676621956.py:10: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n"
     ]
    }
   ],
   "source": [
    "image = imread(\"C:/Users/emmar/Documents/GitHub/VISN/Opdracht_1/flower.jpg\")\n",
    "\n",
    "#grayscale\n",
    "image_gray = rgb2gray(image)\n",
    "\n",
    "#edge detection\n",
    "canny_filter = feature.canny(image_gray, sigma=2.1) #TODO change sigma\n",
    "\n",
    "#gaussian filter                    #for rgb image\n",
    "gaussian_filter = gaussian(image, multichannel=True, sigma=2) #TODO change sigma\n",
    "\n",
    "# viewer = ImageViewer(image)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(image_gray)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(canny_filter)\n",
    "# viewer.show()\n",
    "\n",
    "# viewer = ImageViewer(gaussian_filter)\n",
    "# viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the dataset\n",
    "Dataset used for this project from: https://www.kaggle.com/datasets/grassknoted/asl-alphabet?resource=download\n",
    "Tutorial: https://www.youtube.com/watch?v=j-3vuBynnOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "del\n",
      "nothing\n",
      "space\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"C:/Users/emmar/Documents/GitHub/VISN/Eindopdracht/dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for letter in letters:\n",
    "    #get directory of a certain letter\n",
    "    path = os.path.join(dataset_dir, letter)\n",
    "\n",
    "    #create number for each letter\n",
    "    letter_num = letters.index(letter)\n",
    "    print(letter)\n",
    "    \n",
    "    for image in os.listdir(path):\n",
    "        #get one image\n",
    "        image_array = cv2.imread(os.path.join(path, image), cv2.IMREAD_GRAYSCALE) #TODO gray scale can be added here, do it??\n",
    "\n",
    "        #compress the image to a smaller resolution\n",
    "        #TODO is this needed?? > makes faster\n",
    "        image_size = 50 #TODO this bigger/smaller?\n",
    "        compressed_image_array = cv2.resize(image_array, (image_size, image_size))\n",
    "\n",
    "        #add new image to the training set\n",
    "        training_data.append([compressed_image_array, letter_num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formating the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize the images\n",
    "random.shuffle(training_data)\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "test_data_size = 10000\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "#separate images and labels into the testing dataset\n",
    "#(the first 10.000) and the training dataset (the rest)\n",
    "\n",
    "#TODO can this be done easier? split?\n",
    "for i in range(len(training_data)):\n",
    "    if i < test_data_size:\n",
    "        test_images.append(training_data[i][0])\n",
    "        test_labels.append(training_data[i][1])\n",
    "    else:\n",
    "        train_images.append(training_data[i][0])\n",
    "        train_labels.append(training_data[i][1])\n",
    "\n",
    "#reshape train and test images\n",
    "train_images = np.array(train_images).reshape(-1, image_size, image_size, 1)\n",
    "test_images = np.array(test_images).reshape(-1, image_size, image_size, 1)\n",
    "\n",
    "#normalize the images\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2407/2407 [==============================] - 28s 11ms/step - loss: 2.0181 - accuracy: 0.4194 - val_loss: 1.5459 - val_accuracy: 0.5480\n",
      "Epoch 2/10\n",
      "2407/2407 [==============================] - 30s 13ms/step - loss: 1.2532 - accuracy: 0.6338 - val_loss: 1.1157 - val_accuracy: 0.6841\n",
      "Epoch 3/10\n",
      "2407/2407 [==============================] - 30s 13ms/step - loss: 0.9734 - accuracy: 0.7183 - val_loss: 0.9374 - val_accuracy: 0.7288\n",
      "Epoch 4/10\n",
      "2407/2407 [==============================] - 29s 12ms/step - loss: 0.8039 - accuracy: 0.7688 - val_loss: 0.7953 - val_accuracy: 0.7690\n",
      "Epoch 5/10\n",
      "2407/2407 [==============================] - 28s 12ms/step - loss: 0.6782 - accuracy: 0.8078 - val_loss: 0.7001 - val_accuracy: 0.7913\n",
      "Epoch 6/10\n",
      "2407/2407 [==============================] - 28s 11ms/step - loss: 0.5813 - accuracy: 0.8352 - val_loss: 0.5892 - val_accuracy: 0.8339\n",
      "Epoch 7/10\n",
      "2407/2407 [==============================] - 28s 11ms/step - loss: 0.5059 - accuracy: 0.8575 - val_loss: 0.5324 - val_accuracy: 0.8468\n",
      "Epoch 8/10\n",
      "2407/2407 [==============================] - 28s 12ms/step - loss: 0.4444 - accuracy: 0.8746 - val_loss: 0.4933 - val_accuracy: 0.8535\n",
      "Epoch 9/10\n",
      "2407/2407 [==============================] - 28s 12ms/step - loss: 0.3940 - accuracy: 0.8899 - val_loss: 0.4405 - val_accuracy: 0.8671\n",
      "Epoch 10/10\n",
      "2407/2407 [==============================] - 28s 12ms/step - loss: 0.3532 - accuracy: 0.9016 - val_loss: 0.4113 - val_accuracy: 0.8776\n",
      "313/313 - 1s - loss: 0.4113 - accuracy: 0.8776 - 1s/epoch - 5ms/step\n",
      "0.8776000142097473\n"
     ]
    }
   ],
   "source": [
    "num_filters = 3\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "num_epochs = 10 #TODO change these vars\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=train_images[0].shape),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(29, activation=\"sigmoid\", name=\"dense\"),\n",
    "])\n",
    "\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, to_categorical(train_labels), epochs=num_epochs, validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
